{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Log Processing on GPUs\n",
    "\n",
    "Almost since the coining of the phrase \"big data\", log-processing has been a primary use-case for analytics platforms.\n",
    "\n",
    "Logs are *voluminous*:\n",
    "\n",
    "A single website visit can result in 10s to 100s of log entries, each with lengthy strings of duplicated client information.\n",
    "\n",
    "They're *complex*:\n",
    "Extracting user activities often requires combining multiple records by time and unique session identifier(s).\n",
    "\n",
    "They're *time-sensitive*:\n",
    "When something goes wrong, you need to know quickly.\n",
    "\n",
    "While early big data architectures were oriented towards batch jobs, the focus has shifted to lower-latency solutions. Distributed data processing tools and APIs have made it easier for developers to write _streaming_ applications.\n",
    "\n",
    "Below we provide an example of how to do streaming web-log processing with RAPIDS, Dask, and Streamz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Requisites\n",
    "\n",
    "We assume you're running in a RAPIDS nightly or release container, and thus already have cuDF and Dask installed.\n",
    "\n",
    "Make sure you have [streamz](https://github.com/python-streamz/streamz) installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y streamz ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "For demonstration purposes, we'll use a [publicly available web-log dataset from NASA](http://opensource.indeedeng.io/imhotep/docs/sample-data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib.request, gzip, io\n",
    "\n",
    "data_dir = '/data/'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "url = 'http://indeedeng.github.io/imhotep/files/nasa_19950630.22-19950728.12.tsv.gz'\n",
    "fn = 'logs_noheader.tsv'\n",
    "\n",
    "fileStream = io.BytesIO(urllib.request.urlopen(url).read())\n",
    "\n",
    "# We remove the header line to avoid sending it in some batches and not others\n",
    "with gzip.open(fileStream, 'rb') as f_in, open(data_dir + fn, 'wb') as fout:\n",
    "    # This is a latin character set so we must re-encode it\n",
    "    data = f_in.read().decode('iso-8859-1')\n",
    "    p_data = data.partition('\\n')\n",
    "    names = p_data[0].split()\n",
    "    fout.write(p_data[2].encode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Data\n",
    "\n",
    "The Google SRE HandBook says it's a good idea to track the [4 Golden Signals](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/#xref_monitoring_golden-signals) for any important system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host</th>\n",
       "      <th>logname</th>\n",
       "      <th>time</th>\n",
       "      <th>method</th>\n",
       "      <th>url</th>\n",
       "      <th>response</th>\n",
       "      <th>bytes</th>\n",
       "      <th>referer</th>\n",
       "      <th>useragent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199.72.81.55</td>\n",
       "      <td>-</td>\n",
       "      <td>804571201</td>\n",
       "      <td>GET</td>\n",
       "      <td>/history/apollo/</td>\n",
       "      <td>200</td>\n",
       "      <td>6245</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unicomp6.unicomp.net</td>\n",
       "      <td>-</td>\n",
       "      <td>804571206</td>\n",
       "      <td>GET</td>\n",
       "      <td>/shuttle/countdown/</td>\n",
       "      <td>200</td>\n",
       "      <td>3985</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>199.120.110.21</td>\n",
       "      <td>-</td>\n",
       "      <td>804571209</td>\n",
       "      <td>GET</td>\n",
       "      <td>/shuttle/missions/sts-73/mission-sts-73.html</td>\n",
       "      <td>200</td>\n",
       "      <td>4085</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>burger.letters.com</td>\n",
       "      <td>-</td>\n",
       "      <td>804571211</td>\n",
       "      <td>GET</td>\n",
       "      <td>/shuttle/countdown/liftoff.html</td>\n",
       "      <td>304</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199.120.110.21</td>\n",
       "      <td>-</td>\n",
       "      <td>804571211</td>\n",
       "      <td>GET</td>\n",
       "      <td>/shuttle/missions/sts-73/sts-73-patch-small.gif</td>\n",
       "      <td>200</td>\n",
       "      <td>4179</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   host logname       time method  \\\n",
       "0          199.72.81.55       -  804571201    GET   \n",
       "1  unicomp6.unicomp.net       -  804571206    GET   \n",
       "2        199.120.110.21       -  804571209    GET   \n",
       "3    burger.letters.com       -  804571211    GET   \n",
       "4        199.120.110.21       -  804571211    GET   \n",
       "\n",
       "                                               url  response  bytes  referer  \\\n",
       "0                                 /history/apollo/       200   6245       -1   \n",
       "1                              /shuttle/countdown/       200   3985       -1   \n",
       "2     /shuttle/missions/sts-73/mission-sts-73.html       200   4085       -1   \n",
       "3                  /shuttle/countdown/liftoff.html       304      0       -1   \n",
       "4  /shuttle/missions/sts-73/sts-73-patch-small.gif       200   4179       -1   \n",
       "\n",
       "   useragent  \n",
       "0         -1  \n",
       "1         -1  \n",
       "2         -1  \n",
       "3         -1  \n",
       "4         -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf\n",
    "\n",
    "df = cudf.read_csv(data_dir + fn, sep='\\t', names=names, quoting=3)\n",
    "# The input data has quotation marks which should not be incorretly interpreted as a string with delimiters. Hence quoting is set to 3 (no quoting)\n",
    "df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above doesn't tell us anything about request latency, but we can aggregate it to get a view into traffic, errors, and saturation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "***.novo.dk                        16\n",
       "007.thegap.com                     45\n",
       "01-dynamic-c.wokingham.luna.net    28\n",
       "02-dynamic-c.wokingham.luna.net    13\n",
       "03-dynamic-c.wokingham.luna.net    15\n",
       "Name: host, dtype: int32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate total requests served per host system\n",
    "traffic = df.groupby(['host']).host.count()\n",
    "traffic[traffic > 5].head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "host                     response\n",
       "129.130.115.19           501          1\n",
       "134.57.9.77              501          6\n",
       "163.205.1.45             500         53\n",
       "163.205.16.23            501          1\n",
       "192.83.171.94            501          1\n",
       "cc.newcastle.edu.au      501          1\n",
       "ix-tf1-18.ix.netcom.com  501          1\n",
       "n1032036.ksc.nasa.gov    501          1\n",
       "newcastle03.nbnet.nb.ca  501          2\n",
       "reddragon.ksc.nasa.gov   500          1\n",
       "titan02                  500          4\n",
       "titan02f                 500          4\n",
       "Name: host, dtype: int32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count HTTP error codes per host system\n",
    "errors = df[df['response'] >= 500].groupby(['host', 'response']).host.count()\n",
    "errors.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163.206.137.21        138.230477\n",
       "163.206.89.4          104.978019\n",
       "198.133.29.18         104.475133\n",
       "alyssa.prodigy.com    209.657138\n",
       "e659229.boeing.com    123.248257\n",
       "Name: bytes, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# measure possible saturation of host network cards\n",
    "mb_sent = df.groupby(['host']).bytes.sum()/1000000\n",
    "mb_sent[mb_sent > 100].head().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the above that there are not many errors which is great and we can also see hits per host and total MBs sent per host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single GPU Streaming with RAPIDS and Streamz\n",
    "\n",
    "A single GPU can process a lot of data quickly. Thanks to the Streamz API, it's also easy to do it in streaming fashion.\n",
    "\n",
    "In many streaming systems you return events of interest for ops teams to investigate. That is what we will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "# calculate traffic, errors, and saturation per batch\n",
    "def process_on_gpu(messages):\n",
    "    message_stream = StringIO(\n",
    "            '\\n'.join(msg.decode('utf-8') if isinstance(msg,bytes) else msg for msg in messages)\n",
    "    )\n",
    "    df = cudf.read_csv(message_stream, sep='\\t', names=names, quoting=3)\n",
    "    traffic = df.groupby(['host']).host.count()\n",
    "    errors = df[df['response'] >= 500].groupby(['host', 'response']).host.count()\n",
    "    mb_sent = df.groupby(['host']).bytes.sum()/1000000\n",
    "\n",
    "    # Return - TSV versions of each metric\n",
    "    return {'traffic': traffic[traffic > 200].to_string(), 'errors': errors.to_string(), 'mb_sent': mb_sent[mb_sent > 120].to_string()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime\n",
    "\n",
    "# save each metric type to its own file, instead of dumping lots of output to Jupyter\n",
    "def save_to_file(events):\n",
    "    dt = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    with open(data_dir + 'traffic.txt', 'w+') as fp:\n",
    "        fp.write(str(dt) + ':' + events['traffic'] + '\\n')\n",
    "    with open(data_dir + 'errors.txt', 'w+') as fp:\n",
    "        fp.write(str(dt) + ':' + events['errors'] + '\\n')\n",
    "    with open(data_dir + 'mb_sent.txt', 'w+') as fp:\n",
    "        fp.write(str(dt) + ':' + events['mb_sent'] + '\\n')\n",
    "    print(str(dt) + ': metrics batch written..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that the function above opens the file in overwrite mode for simplicity (which means the data in the file would correspond to the last batch only). Typically a workflow would append to the existing file, write to a seperate file with a timestamp based filename or produce to another kafka topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamz import Stream\n",
    "\n",
    "# setup the stream\n",
    "# Streamz allows streaming directly from a text file\n",
    "source = Stream.from_textfile(data_dir + fn)\n",
    "# process 250k lines per batch\n",
    "out = source.partition(250000).map(process_on_gpu).sink(save_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-29 18:14:30: metrics batch written..\n",
      "2019-07-29 18:14:33: metrics batch written..\n",
      "2019-07-29 18:14:37: metrics batch written..\n",
      "2019-07-29 18:14:40: metrics batch written..\n",
      "2019-07-29 18:14:43: metrics batch written..\n",
      "2019-07-29 18:14:47: metrics batch written..\n",
      "2019-07-29 18:14:50: metrics batch written..\n"
     ]
    }
   ],
   "source": [
    "source.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Log:\n",
      "2019-07-29 18:14:50:('134.57.9.77', 501)    5\n",
      "Name: host, dtype: int32\n",
      "\n",
      "Traffic Log:\n",
      "2019-07-29 18:14:50:128.159.105.240     212\n",
      "163.205.1.45     426\n",
      "163.205.156.16     308\n",
      "163.205.180.17     317\n",
      "163.206.137.21     308\n",
      "\n",
      "MB Sent Log:\n",
      "2019-07-29 18:14:50:<empty Series of dtype=float64>\n"
     ]
    }
   ],
   "source": [
    "!echo \"Error Log:\"\n",
    "!head -n5 {data_dir}errors.txt\n",
    "!echo \"\\nTraffic Log:\"\n",
    "!head -n5 {data_dir}traffic.txt\n",
    "!echo \"\\nMB Sent Log:\"\n",
    "!head -n5 {data_dir}mb_sent.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Streamz to multiple GPUs with Dask & Kafka\n",
    "\n",
    "As opposed to streaming from files a very common pattern is to read from distributed log systems like Apache Kafka.\n",
    "\n",
    "The below example assumes you have a running Kafka instance/cluster.\n",
    "\n",
    "For help setting up your own, follow the [Kafka Quickstart guide](http://kafka.apache.org/quickstart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "# create a Dask cluster with 1 worker per GPU\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamz uses the [python-confluent-kafka](https://github.com/confluentinc/confluent-kafka-python) library for handling interactions with kafka. Make sure this library installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y python-confluent-kafka "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamz import Stream\n",
    "import confluent_kafka\n",
    "\n",
    "# Kafka specific configurations\n",
    "topic = \"haproxy-topic\"\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "consumer_conf = {'bootstrap.servers': bootstrap_servers, 'group.id': 'custreamz', 'session.timeout.ms': 60000}\n",
    "\n",
    "stream = Stream.from_kafka_batched(topic, consumer_conf, poll_interval='1s', npartitions=1, asynchronous=True, dask=False)\n",
    "final_output = stream.map(process_on_gpu).sink(save_to_file)\n",
    "stream.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, upstream applications produce messages to Kafka. For the sake of a self contained example you can experiment with, we'll use the Confluent Kafka library to produce messages that our Streamz app immediately consumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-29 18:15:11: metrics batch written..\n",
      "Producer queue is now empty!\n",
      "2019-07-29 18:15:17: metrics batch written..\n",
      "2019-07-29 18:15:32: metrics batch written..\n"
     ]
    }
   ],
   "source": [
    "producer = confluent_kafka.Producer({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "with open(data_dir+fn, 'rb') as fp:\n",
    "    for line in fp.readlines():\n",
    "        try:\n",
    "            producer.produce(topic, line)\n",
    "        except BufferError:\n",
    "            # Wait for the specified timeout as the queue is full\n",
    "            producer.flush(0.2)\n",
    "\n",
    "producer.flush()\n",
    "print(\"Producer queue is now empty!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Log:\n",
      "2019-07-29 18:15:32:('129.130.115.19', 501)    1\n",
      "('134.57.9.77', 501)    5\n",
      "('163.205.16.23', 501)    1\n",
      "('192.83.171.94', 501)    1\n",
      "('cc.newcastle.edu.au', 501)    1\n",
      "\n",
      "Traffic Log:\n",
      "2019-07-29 18:15:32:128.158.26.178      235\n",
      "128.158.42.150      281\n",
      "128.158.54.14      206\n",
      "128.159.105.240      589\n",
      "128.159.111.141      376\n",
      "\n",
      "MB Sent Log:\n",
      "2019-07-29 18:15:32:piweba1y.prodigy.com    148.073398\n",
      "piweba3y.prodigy.com     229.52305\n",
      "piweba4y.prodigy.com    213.940041\n",
      "Name: bytes, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "!echo \"Error Log:\"\n",
    "!head -n5 {data_dir}errors.txt\n",
    "!echo \"\\nTraffic Log:\"\n",
    "!head -n5 {data_dir}traffic.txt\n",
    "!echo \"\\nMB Sent Log:\"\n",
    "!head -n5 {data_dir}mb_sent.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
